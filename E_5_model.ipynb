{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"./figures/logo-hi-paris-retina.png\" alt=\"Logo\" width=\"280\" height=\"180\">\n",
    "  </a>\n",
    "\n",
    "  <h3 align=\"center\">Data Science Bootcamp</h3>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors : Yann Berthelot, Florian Bettini, Laure-Amélie Colin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Val / Test Split\n",
    "The **train-validation-test** split is a technique for training and evaluating the performance of a machine learning algorithm.\n",
    "\n",
    "The **procedure** involves taking a dataset and dividing it into three subsets:\n",
    "- The **train** subset is used to fit the model and is referred to as the training dataset. You should not evaluate the performance of the model on this train set,\n",
    "- The **validation** subset is used to tune (select the best) hyperparameters of an algorithm. For example the *max_depth* for a regression tree. It is used to compare different versions of the model with different hyperparameters, but not to evaluate it.\n",
    "- The **test** subset is not used to train the model; it is only used at the end to evaluate the performances of the model. For a purist, once you 'opened' the test set, you should not modify the model anymore.  \n",
    "\n",
    "Test set cannot be used for validation as it could lead to overifitting while selecting amongst different models with different hyperparameters. It should only be used for evaluation and not for tuning.\n",
    "\n",
    "**Strategy**\n",
    "   - Define a test set, and separate the remaining between train and validation.\n",
    "   - Generally they represent respectively 70% | 15% | 15% of the initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/train_test_1.png\" width=700 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However there is a specificity to our problem as we are dealing with timeseries. By shuffling the data and randomly selecting examples to build our different sets we would be facing a problem of leaking future information with past information :  \n",
    "If we randomly selected data points for our training set and test set we would have fires in the train set that would be of later dates than some in the test set. This would mean that the model learnt from future informations and thus is \"cheating\" since it already knows some of what happens next (e.g. there was a serie of fire afterwards). This is called leaking and we want to avoid it.\n",
    "\n",
    "As a consequence, we will be using a different type of split made for time series : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/TimeSeriesSplit.png\" width=700 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines 0 to 3 are our different split. The data is organized in chronological order. We first use a portion of the data to train, and then test on the following portion of data (line 0). We then repeat this process by adding the previous testing set to our training set and using the following portion as our testing set. We then repeat this operation until we have covered the whole dataset.  \n",
    "Some small adjusment which can be made is to add a \"gap\" between the training and testing set. Since we are dealing with time series, following samples can be highly dependent (e.g. we have two consecutive data points in the same part of the country, the first fire having created the second). If we split between this two point we would have the illusion of testing our model on independent data while we would have been lucky on this example. In order to prevent this effect of dependence we add a gap of discarded data between the training set and testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/TimeSeriesGap.png\" width=700 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features / Target Split\n",
    "It is a rather practical approach because generally the algorithms of machine learning ask for the features on the one hand and the target on the other hand.\n",
    "\n",
    "<img src=\"./figures/split_columns.png\" width=700 height=500 />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Bias / Variance Trade-off](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
    "\n",
    "<img src=\"./figures/Overfitting.PNG\" width=600 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{UNDERFITTING :}$ The model is too simple to capture the relationships between the data\n",
    "\n",
    "*Solutions*:\n",
    "- Introduce more features\n",
    "- Increase model complexity\n",
    "\n",
    "$\\textbf{OVERFITTING :}$ The model is too complex and sticks too closely to the training data\n",
    "\n",
    "*Solutions*:\n",
    "\n",
    "- Decrease model complexity\n",
    "- Include more data\n",
    "- Use regularization\n",
    "\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "The main problem with decision trees is their large variance: a tiny error at the top of the tree is propagated all the way down the tree and it gets worse quickly. To stabilize the tree's predictions, we prefer to generate a set of trees, a forest and this algorithm is called *Random Forest*.\n",
    "\n",
    "To create a random forest with B trees, we proceed as follows:\n",
    "\n",
    "- For i ranging from 1 to B:\n",
    "  - We draw randomly with replacement a sub-sample of the data size $ n <n _ {\\ text {train}} $\n",
    "  - We randomly draw a subsample of features of size m with in general $ m \\leq \\sqrt {p} $\n",
    "  - On this new dataset composed of n examples and m features, we train a decision tree of fixed max depth\n",
    "- We thus obtain $ B $ decision trees. If we denote by $ f1, ..., fB $ the prediction functions of each tree, then in regression, the decision function of the forest $ f_ {RF} $ will be:\n",
    "\n",
    "$f_{RF} (x) = \\frac{1}{B} \\sum_{i = 1, ..., B} f_i (x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/RF.png\" width=900 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mains parameters\n",
    "\n",
    "- *n_estimators* : number of trees in the foreset\n",
    "\n",
    "- *max_features* : max number of features considered for splitting a node\n",
    "\n",
    "- *max_depth* : max number of levels in each decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective of this lab\n",
    "======\n",
    "\n",
    "Experiment with multiple models and hyperparameters and select the one with the best score.\n",
    "Use Cross-validation (among other methods) to avoid overfitting.\n",
    "Use explainability to understand and present the model's predictions.\n",
    "\n",
    "We will use the [sklearn package](https://scikit-learn.org/stable/) for models and various metrics. You also have the possibility to use the [XGBoost](https://xgboost.readthedocs.io/en/stable/) model if you feel confident with it, it has the nice property of allowing to use the same methods as sklearn models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies with previous Labs\n",
    "\n",
    "This lab uses one dataset from a previous Lab:\n",
    "- a dataset ready to use for an ML model `./data/3_input_model/input_model.csv`\n",
    "\n",
    "You can either:\n",
    "- [preferred option] start over from the work that you produced\n",
    "- or take pre-processed datasets located in `./data/9_helper_datasets`. In that case:\n",
    "    - `./data/9_helper_datasets/input_model.csv` should be **copied** (not deleted) to `./data/3_input_model/input_model.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from xgboost) (1.6.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from xgboost) (1.22.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/slundberg/shap.git\n",
      "  Cloning https://github.com/slundberg/shap.git to /tmp/pip-req-build-pwbgk0ye\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/slundberg/shap.git /tmp/pip-req-build-pwbgk0ye\n",
      "  Resolved https://github.com/slundberg/shap.git to commit 45b85c1837283fdaeed7440ec6365a886af4a333\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from shap==0.41.0) (1.2.5)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.8/site-packages (from shap==0.41.0) (1.6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from shap==0.41.0) (1.22.1)\n",
      "Requirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.8/site-packages (from shap==0.41.0) (0.0.7)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.8/site-packages (from shap==0.41.0) (0.53.1)\n",
      "Requirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.8/site-packages (from shap==0.41.0) (21.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from shap==0.41.0) (1.6.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from shap==0.41.0) (1.1.1)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /opt/conda/lib/python3.8/site-packages (from shap==0.41.0) (4.64.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>20.9->shap==0.41.0) (3.0.4)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba->shap==0.41.0) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba->shap==0.41.0) (59.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->shap==0.41.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->shap==0.41.0) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->shap==0.41.0) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->shap==0.41.0) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->shap==0.41.0) (1.16.0)\n",
      "Building wheels for collected packages: shap\n",
      "  Building wheel for shap (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for shap: filename=shap-0.41.0-cp38-cp38-linux_x86_64.whl size=570399 sha256=cf53a38e78c241cc6eaa84b1e443b0a0f1181bee1745d0675ee4b43190aba273\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-39zlzvjk/wheels/43/d4/b7/852c8c4711da053ae2ad230f1a8b270ca2ad3fc26ab00b1a03\n",
      "Successfully built shap\n",
      "Installing collected packages: shap\n",
      "  Attempting uninstall: shap\n",
      "    Found existing installation: shap 0.40.0\n",
      "    Uninstalling shap-0.40.0:\n",
      "      Successfully uninstalled shap-0.40.0\n",
      "Successfully installed shap-0.41.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install git+https://github.com/slundberg/shap.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "\n",
    "# Typing and error catching\n",
    "from functools import partial\n",
    "from typing import Tuple, Any, Union\n",
    "from shap.utils._exceptions import InvalidModelError\n",
    "MODEL_TYPE = Union[LogisticRegression, RandomForestClassifier, XGBClassifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Get Train, val, test, and pred datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "data = pd.read_csv(\"./data/3_input_model/input_model.csv\", parse_dates=[\"DISCOVERY_DATE\"])\n",
    "min_year_pred = \"2015-01-01\"\n",
    "target_col = \"FIRE\"\n",
    "ratio = 0.75\n",
    "\n",
    "# find max number of occurrence per day\n",
    "max_occ_day = data.groupby(\"DISCOVERY_DATE\").agg({\"STATE\":\"count\"}).max().values[0]\n",
    "\n",
    "# split train and prediction datasets\n",
    "X_pred = data[data[\"DISCOVERY_DATE\"] >= min_year_pred].set_index([\"DISCOVERY_DATE\", \"STATE\"]).copy()\n",
    "X_pred.drop(columns=[\"FIRE\"], inplace=True) # get features for predictions\n",
    "data = data[data[\"DISCOVERY_DATE\"] < min_year_pred].copy() # get train data (features and target values)\n",
    "data[target_col] = data[target_col].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_target_split(df:pd.DataFrame, col:str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Split features and target values. \n",
    "    It is meant to help you in the next function, you can skip if you feel like it\n",
    "\n",
    "    Input:\n",
    "    data (pd.DataFrame) : input DataFrame with features and target values\n",
    "    col (str) : name of the target value column\n",
    "\n",
    "    Output:\n",
    "    X (pd.DataFrame): DataFrame with features\n",
    "    y (pd.DataFrame): DataFrame with target values\n",
    "    '''\n",
    "    #################\n",
    "    y = data[col]\n",
    "    X = data.drop(columns=[col], inplace=False)\n",
    "    ################# \n",
    "\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test features_target_split\n",
    "target_col = \"FIRE\"\n",
    "X_train, y_train = features_target_split(data, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>FIRE</th>\n",
       "      <th>tmax_mean</th>\n",
       "      <th>tmax_max</th>\n",
       "      <th>tmax_min</th>\n",
       "      <th>tmin_mean</th>\n",
       "      <th>tmin_max</th>\n",
       "      <th>tmin_min</th>\n",
       "      <th>prcp_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>SD</th>\n",
       "      <th>TN</th>\n",
       "      <th>TX</th>\n",
       "      <th>UT</th>\n",
       "      <th>VA</th>\n",
       "      <th>VT</th>\n",
       "      <th>WA</th>\n",
       "      <th>WI</th>\n",
       "      <th>WV</th>\n",
       "      <th>WY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>MN</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>MI</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>MO</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>IL</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75967</th>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>PR</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75968</th>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>RI</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75969</th>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75970</th>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>MA</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75971</th>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>DE</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75972 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DISCOVERY_DATE STATE  FIRE  tmax_mean   tmax_max   tmax_min  tmin_mean  \\\n",
       "0         2011-01-01    AK     0  66.646709  69.688094  62.859138  45.593077   \n",
       "1         2011-01-01    MN     0  14.000000  26.000000   7.000000   0.333333   \n",
       "2         2011-01-01    MI     0  54.000000  55.000000  53.000000  20.500000   \n",
       "3         2011-01-01    MO     1  27.000000  27.000000  27.000000  11.000000   \n",
       "4         2011-01-01    IL     0  38.000000  41.000000  35.000000  16.250000   \n",
       "...              ...   ...   ...        ...        ...        ...        ...   \n",
       "75967     2014-12-31    PR     0  66.646709  69.688094  62.859138  45.593077   \n",
       "75968     2014-12-31    RI     0  66.646709  69.688094  62.859138  45.593077   \n",
       "75969     2014-12-31    VT     0  66.646709  69.688094  62.859138  45.593077   \n",
       "75970     2014-12-31    MA     0  27.000000  30.000000  24.000000  15.500000   \n",
       "75971     2014-12-31    DE     0  66.646709  69.688094  62.859138  45.593077   \n",
       "\n",
       "        tmin_max   tmin_min  prcp_mean  ...  SD  TN  TX  UT  VA  VT  WA  WI  \\\n",
       "0      48.701341  41.716747   0.099078  ...   0   0   0   0   0   0   0   0   \n",
       "1       1.000000   0.000000   0.006667  ...   0   0   0   0   0   0   0   0   \n",
       "2      24.000000  19.000000   0.182500  ...   0   0   0   0   0   0   0   0   \n",
       "3      11.000000  11.000000   0.000000  ...   0   0   0   0   0   0   0   0   \n",
       "4      21.000000  13.000000   0.000000  ...   0   0   0   0   0   0   0   0   \n",
       "...          ...        ...        ...  ...  ..  ..  ..  ..  ..  ..  ..  ..   \n",
       "75967  48.701341  41.716747   0.099078  ...   0   0   0   0   0   0   0   0   \n",
       "75968  48.701341  41.716747   0.099078  ...   0   0   0   0   0   0   0   0   \n",
       "75969  48.701341  41.716747   0.099078  ...   0   0   0   0   0   1   0   0   \n",
       "75970  20.000000  11.000000   0.000000  ...   0   0   0   0   0   0   0   0   \n",
       "75971  48.701341  41.716747   0.099078  ...   0   0   0   0   0   0   0   0   \n",
       "\n",
       "       WV  WY  \n",
       "0       0   0  \n",
       "1       0   0  \n",
       "2       0   0  \n",
       "3       0   0  \n",
       "4       0   0  \n",
       "...    ..  ..  \n",
       "75967   0   0  \n",
       "75968   0   0  \n",
       "75969   0   0  \n",
       "75970   0   0  \n",
       "75971   0   0  \n",
       "\n",
       "[75972 rows x 74 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>FIRE</th>\n",
       "      <th>tmax_mean</th>\n",
       "      <th>tmax_max</th>\n",
       "      <th>tmax_min</th>\n",
       "      <th>tmin_mean</th>\n",
       "      <th>tmin_max</th>\n",
       "      <th>tmin_min</th>\n",
       "      <th>prcp_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>SD</th>\n",
       "      <th>TN</th>\n",
       "      <th>TX</th>\n",
       "      <th>UT</th>\n",
       "      <th>VA</th>\n",
       "      <th>VT</th>\n",
       "      <th>WA</th>\n",
       "      <th>WI</th>\n",
       "      <th>WV</th>\n",
       "      <th>WY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>MN</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>MI</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>MO</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>IL</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56987</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>PR</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56988</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>RI</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56989</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56990</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>MA</td>\n",
       "      <td>0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56991</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>DE</td>\n",
       "      <td>0</td>\n",
       "      <td>66.646709</td>\n",
       "      <td>69.688094</td>\n",
       "      <td>62.859138</td>\n",
       "      <td>45.593077</td>\n",
       "      <td>48.701341</td>\n",
       "      <td>41.716747</td>\n",
       "      <td>0.099078</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56992 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DISCOVERY_DATE STATE  FIRE  tmax_mean   tmax_max   tmax_min  tmin_mean  \\\n",
       "0         2011-01-01    AK     0  66.646709  69.688094  62.859138  45.593077   \n",
       "1         2011-01-01    MN     0  14.000000  26.000000   7.000000   0.333333   \n",
       "2         2011-01-01    MI     0  54.000000  55.000000  53.000000  20.500000   \n",
       "3         2011-01-01    MO     1  27.000000  27.000000  27.000000  11.000000   \n",
       "4         2011-01-01    IL     0  38.000000  41.000000  35.000000  16.250000   \n",
       "...              ...   ...   ...        ...        ...        ...        ...   \n",
       "56987     2013-12-31    PR     0  66.646709  69.688094  62.859138  45.593077   \n",
       "56988     2013-12-31    RI     0  66.646709  69.688094  62.859138  45.593077   \n",
       "56989     2013-12-31    VT     0  66.646709  69.688094  62.859138  45.593077   \n",
       "56990     2013-12-31    MA     0  23.500000  26.000000  21.000000  12.000000   \n",
       "56991     2013-12-31    DE     0  66.646709  69.688094  62.859138  45.593077   \n",
       "\n",
       "        tmin_max   tmin_min  prcp_mean  ...  SD  TN  TX  UT  VA  VT  WA  WI  \\\n",
       "0      48.701341  41.716747   0.099078  ...   0   0   0   0   0   0   0   0   \n",
       "1       1.000000   0.000000   0.006667  ...   0   0   0   0   0   0   0   0   \n",
       "2      24.000000  19.000000   0.182500  ...   0   0   0   0   0   0   0   0   \n",
       "3      11.000000  11.000000   0.000000  ...   0   0   0   0   0   0   0   0   \n",
       "4      21.000000  13.000000   0.000000  ...   0   0   0   0   0   0   0   0   \n",
       "...          ...        ...        ...  ...  ..  ..  ..  ..  ..  ..  ..  ..   \n",
       "56987  48.701341  41.716747   0.099078  ...   0   0   0   0   0   0   0   0   \n",
       "56988  48.701341  41.716747   0.099078  ...   0   0   0   0   0   0   0   0   \n",
       "56989  48.701341  41.716747   0.099078  ...   0   0   0   0   0   1   0   0   \n",
       "56990  14.000000  10.000000   0.000000  ...   0   0   0   0   0   0   0   0   \n",
       "56991  48.701341  41.716747   0.099078  ...   0   0   0   0   0   0   0   0   \n",
       "\n",
       "       WV  WY  \n",
       "0       0   0  \n",
       "1       0   0  \n",
       "2       0   0  \n",
       "3       0   0  \n",
       "4       0   0  \n",
       "...    ..  ..  \n",
       "56987   0   0  \n",
       "56988   0   0  \n",
       "56989   0   0  \n",
       "56990   0   0  \n",
       "56991   0   0  \n",
       "\n",
       "[56992 rows x 74 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['DISCOVERY_DATE']<=data.iloc[int(0.75 * data.shape[0]), data.columns.get_loc(\"DISCOVERY_DATE\")]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df:pd.DataFrame, col:str, ratio: float = 0.75) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Split inputs into 2 sets of data: training (train) and test (test).\n",
    "    Each set of data is splitted into features (X) and target values (y).\n",
    "    Be careful not to split with shuffling as we need to keep the chronological order\n",
    "    \n",
    "\n",
    "    Input:\n",
    "    data (pd.DataFrame) : input DataFrame with features and target values\n",
    "    col (str) : name of the target value column\n",
    "    ratio (float) : split ratio, between 0 and 1, to split train and validation data\n",
    "\n",
    "    Output:\n",
    "    X_train (pd.DataFrame): DataFrame for training with features\n",
    "    y_train (pd.DataFrame): DataFrame for training with target values\n",
    "    X_test (pd.DataFrame): DataFrame for testing with features\n",
    "    y_test (pd.DataFrame): DataFrame for testing with target values\n",
    "    '''\n",
    "    # spint train and test sets\n",
    "    df = df.sort_values(by=[\"DISCOVERY_DATE\"])\n",
    "    index_ratio = int(ratio * df.shape[0]) # find the row number where we want to split\n",
    "    split_date = df.iloc[index_ratio, df.columns.get_loc(\"DISCOVERY_DATE\")] # find the corresponding data\n",
    "    data_train = pd.DataFrame(df[df['DISCOVERY_DATE']<=split_date])\n",
    "    data_test = pd.DataFrame(df[df['DISCOVERY_DATE']>split_date])\n",
    "\n",
    "    # split between features and target values\n",
    "    X_train, y_train = features_target_split(data_train, col)\n",
    "    X_test, y_test = features_target_split(data_test, col)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, val and test sets\n",
    "X_train_val, y_train_val, X_test, y_test = train_test_split(data, target_col, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Use Train data for parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DISCOVERY_DATE    datetime64[ns]\n",
       "STATE                     object\n",
       "tmax_mean                float64\n",
       "tmax_max                 float64\n",
       "tmax_min                 float64\n",
       "                       ...      \n",
       "VT                         int64\n",
       "WA                         int64\n",
       "WI                         int64\n",
       "WV                         int64\n",
       "WY                         int64\n",
       "Length: 73, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_predict(model:MODEL_TYPE, X_train:pd.DataFrame, y_train:pd.DataFrame, X_val:pd.DataFrame) -> Tuple[pd.DataFrame, Any]:\n",
    "    '''\n",
    "    Create a model, fit it on X_train and y_train, and predict the target values from X_val\n",
    "\n",
    "    Input:\n",
    "    model (MODEL_TYPE) : The model to fit and to then use for predictions on the validation set\n",
    "    X_train (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train (pd.DataFrame) : input DataFrame with target values for training\n",
    "    X_val (pd.DataFrame) : input DataFrame with features for validation\n",
    "\n",
    "    Output:\n",
    "    y_pre (pd.DataFrame): predictions based on the features from X_val\n",
    "    model : trained model\n",
    "    '''\n",
    "    #################\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pre = model.predict(X_val)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return y_pre, model\n",
    "    #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Enlever DISCOVERY_DATE et STATE :\n",
    "- Pour DISCOVERY_DATE : extraire le mois, le jour du mois, le jour de la semaine, la semaine de l'année... pas l'année car c'est pas compréhensible pour le modèle. Il aura des infos concernant les années avec les features nb de feux il y a 1 an, 2 ans...\n",
    "- Pour STATE, one-hot encoding\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test model_fit_predict\u001b[39;00m\n\u001b[1;32m      3\u001b[0m random_forest \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m----> 4\u001b[0m y_pre, trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_forest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36mmodel_fit_predict\u001b[0;34m(model, X_train, y_train, X_val)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mCreate a model, fit it on X_train and y_train, and predict the target values from X_val\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mmodel : trained model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#################\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m y_pre \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pre, model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:331\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 331\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/generic.py:1899\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'Timestamp'"
     ]
    }
   ],
   "source": [
    "# Test model_fit_predict\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "y_pre, trained_model = model_fit_predict(random_forest, X_train_val, y_train_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(y_true:pd.DataFrame, y_pre:pd.DataFrame) -> dict:\n",
    "    '''\n",
    "    Return a dictionary with keys corresponding to score name and values corresponding to the associated score\n",
    "\n",
    "    Input:\n",
    "    y_true (pd.DataFrame) : input DataFrame with true labels\n",
    "    y_pre (pd.DataFrame) : input DataFrame with predicted labels\n",
    "\n",
    "    Output:\n",
    "    (dict): output dictionary with scores\n",
    "    '''\n",
    "    return {\n",
    "        \"f1-micro\": f1_score(y_true, y_pre, average=\"micro\"),\n",
    "        \"f1-macro\": f1_score(y_true, y_pre, average=\"macro\"),\n",
    "        \"f1-weighted\": f1_score(y_true, y_pre, average=\"weighted\"),\n",
    "        \"accuracy\": accuracy_score(y_true, y_pre)\n",
    "    }\n",
    "\n",
    "def print_scoring(scores:dict) -> None:\n",
    "    '''\n",
    "    Print scores from a dictionary\n",
    "\n",
    "    Input:\n",
    "    scores (dict) : dictionary with keys corresponding to score name and values corresponding to the associated score\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    for name, score in scores.items():\n",
    "        print(f\"{name}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scoring\n",
    "scores = scoring(y_test, y_pre)\n",
    "print_scoring(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test:pd.DataFrame, y_pre:pd.DataFrame, nb_values:int) -> None:\n",
    "    '''\n",
    "    Plot the confusion matrix based on the provided true and predicted labels\n",
    "\n",
    "    Input:\n",
    "    y_true (pd.DataFrame) : input DataFrame with true labels\n",
    "    y_pre (pd.DataFrame) : input DataFrame with predicted labels\n",
    "    nb_values (int) : nb of target values\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    s = sns.heatmap(\n",
    "        confusion_matrix(y_test, y_pre),\n",
    "        xticklabels=range(nb_values),\n",
    "        yticklabels=range(nb_values),\n",
    "        annot=True,\n",
    "        cmap='Blues',\n",
    "        fmt='g',\n",
    "        cbar=False\n",
    "    )\n",
    "    s.set(xlabel='True label', ylabel='Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pre, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_features_importance(model: MODEL_TYPE, cols:list) -> None:\n",
    "    '''\n",
    "    Plot feature_importance from a random forest model\n",
    "\n",
    "    Input:\n",
    "    model (MODEL_TYPE) : Model for which to compute the feature importance\n",
    "    cols (list) : input DataFrame with true labels\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    # get feature importance from model\n",
    "    importances = model.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=cols).sort_values(ascending=False)[:10]\n",
    "    # plot results\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances.plot.bar(ax=ax)\n",
    "    ax.set_title(\"Feature importances\")\n",
    "    fig.tight_layout()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test random forest feature importance\n",
    "rf_features_importance(random_forest, cols=X_train_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score(model: MODEL_TYPE, X_train_val:pd.DataFrame, y_train_val:pd.DataFrame, gap:int) -> Tuple[MODEL_TYPE, dict]:\n",
    "    '''\n",
    "    Cross validation for time series, with 5 splits.\n",
    "\n",
    "    Input:\n",
    "    X_train_val (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train_val (pd.DataFrame) : input DataFrame with target values for training\n",
    "    gap (int) : number of entries between 2 sets during the cross-validation\n",
    "\n",
    "    Output:\n",
    "    model: trained model\n",
    "    '''\n",
    "    # cross-validation for time series\n",
    "    tscv = TimeSeriesSplit(n_splits=3, gap=gap)\n",
    "    y_pre, y_val, trained_model = None, None, None\n",
    "    scores_history = []\n",
    "    i = 0\n",
    "    for train_index, val_index in tscv.split(X_train_val):\n",
    "        i += 1 # increase iteration\n",
    "        # get datasets (train and val)\n",
    "        train_index = list(train_index)\n",
    "        val_index = list(val_index)\n",
    "        X_train, X_val = X_train_val.iloc[train_index, :], X_train_val.iloc[val_index, :]\n",
    "        y_train, y_val = y_train_val.iloc[train_index, :], y_train_val.iloc[val_index, :]\n",
    "        # fit model and predict y_val\n",
    "        y_pre, trained_model = model_fit_predict(model, X_train, y_train, X_val)\n",
    "        # scoring\n",
    "        scores = scoring(y_val, y_pre)\n",
    "        scores_history.append(scores)\n",
    "        print(\"\")\n",
    "        print(f\"Step {i}\")\n",
    "        print_scoring(scores)\n",
    "\n",
    "    return trained_model, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression, logistic_scores = cross_validation_score(logistic_regression, X_train_val, y_train_val, gap = max_occ_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model on X_train_val and y_train_val\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest, random_forest_scores = cross_validation_score(random_forest, X_train_val, y_train_val, gap = max_occ_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "xgb_model, xgb_scores = cross_validation_score(xgb_model, X_train_val, y_train_val, gap = max_occ_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_params = {\"n_estimators\":[100,200], 'min_samples_split': [2, 5, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest = GridSearchCV(random_forest, random_forest_params, scoring=None, n_jobs=None, refit=True, cv=TimeSeriesSplit(n_splits=3, gap=max_occ_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "        'min_child_weight': [5, 10],\n",
    "        'gamma': [1, 2],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = GridSearchCV(xgb_model, xgb_params, scoring=None, n_jobs=None, refit=True, cv=TimeSeriesSplit(n_splits=3, gap=max_occ_day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Test the model on the Test set, and refit the model on the entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be done only few times (risk of overfitting)\n",
    "def score_test_set(model:Any, X_train_val:pd.DataFrame, y_train_val:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Fit the model on the entire X_train_val and y_train_val data, and predict values for the test set\n",
    "\n",
    "    Input:\n",
    "    model (MODEL_TYPE) : model used for training the the previous section of the notebook\n",
    "    X_train_val (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train_val (pd.DataFrame) : input DataFrame with target values for training\n",
    "    X_test (pd.DataFrame) : input DataFrame with features for testing\n",
    "    y_test (pd.DataFrame) : input DataFrame with target values for testing\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    # fit model on entire X_train_val, y_train_val datasets\n",
    "    model.fit(X_train_val, y_train_val.values.ravel())\n",
    "    y_pred = model.predict(X_test)\n",
    "    # score test set\n",
    "    scores = scoring(y_test, y_pred)\n",
    "    print_scoring(scores)\n",
    "    # plot the last confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred, nb_values=2)\n",
    "    # plot features importance\n",
    "    cols = X_train_val.columns # get columns names\n",
    "    if 'feature_importances_' in dir(model):\n",
    "        rf_features_importance(model, cols) # plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test_set(random_forest, X_train_val, y_train_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in (logistic_regression, xgb_model, random_forest, best_random_forest, best_xgb):\n",
    "    print(model.__repr__().split('(')[0])\n",
    "    score_test_set(model, X_train_val, y_train_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Create the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_all_data(model: MODEL_TYPE, X_train_val:pd.DataFrame, y_train_val:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame):\n",
    "    '''\n",
    "    Fit the model on the entire X_train_val and X_test features\n",
    "\n",
    "    Input:\n",
    "    model : model used for training the the previous section of the notebook\n",
    "    X_train_val (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train_val (pd.DataFrame) : input DataFrame with target values for training\n",
    "    X_test (pd.DataFrame) : input DataFrame with features for testing\n",
    "    y_test (pd.DataFrame) : input DataFrame with target values for testing\n",
    "\n",
    "    Output:\n",
    "    model : model trained on all available data\n",
    "    '''\n",
    "    # fit model on entire X_train_val, y_train_val datasets\n",
    "    X = pd.concat([X_train_val, X_test])\n",
    "    y = pd.concat([y_train_val, y_test])\n",
    "    model.fit(X, y.values.ravel())\n",
    "    return model\n",
    "    \n",
    "\n",
    "def save_predictions(X_pred:pd.DataFrame, model:MODEL_TYPE, filename:str) -> None:\n",
    "    '''\n",
    "    Save predictions (year 2015) to csv format, based on a provided pre-trained model, and features for predictions X_pred\n",
    "\n",
    "    Input:\n",
    "    X_pred (pd.DataFrame) : input DataFrame with features for predictions\n",
    "    model (MODEL_TYPE) : input pre-trained model, which has a predict method\n",
    "    filename (str) : Name of the file for submission\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    # predict target values\n",
    "    y_pred = model.predict(X_pred)\n",
    "    # create a DataFrame with results\n",
    "    submission = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        index=X_pred.index,\n",
    "        columns=[\"CAUSE_CODE\"]\n",
    "    ).reset_index()\n",
    "    # save to csv\n",
    "    submission.to_csv(f\"./data/4_predictions/{filename}.csv\", index=False)\n",
    "\n",
    "def save_model(model: MODEL_TYPE, file_name:str) -> None:\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    dump(model, f'./models/{file_name}.joblib') \n",
    "\n",
    "def load_model(file_name:str) -> MODEL_TYPE:\n",
    "    return load(f'./models/{file_name}.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to csv\n",
    "filename = \"submission_group_X\"\n",
    "model = fit_model_all_data(model, X_train_val, y_train_val, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions(X_pred, model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if necessary, for example if you want to close this notebook and continue fine-tuning later\n",
    "save_model(model, file_name=\"model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if necessary for further experiments\n",
    "model = load_model('model_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability\n",
    "\n",
    "  \n",
    "Now that we have a working model, we can try to understand how its predictions are made. Beyond feature importance we can study [Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html). They allow us to see the importance of each feature as well as how each feature impacts the predictions towards a prediction or the other depending on the feature value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shapley_values(model:MODEL_TYPE)->None:\n",
    "    '''\n",
    "    Plot the shapley graph of the given model.\n",
    "    ##########################################################################\n",
    "    WARNING : Do not use it on random forest as it takes a huge amout of time.\n",
    "    ##########################################################################\n",
    "    Input:\n",
    "    model (MODEL_TYPE) : Model for which to explain the predictions\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap.summary_plot(shap_values, X_test)\n",
    "    except (InvalidModelError, TypeError) as error:\n",
    "        explainer = shap.Explainer(model, X_test)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap.summary_plot(shap_values, X_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_values(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_values(logistic_regression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f17db48aa853750bfee38181acc93506773951f4f6f179b65dfa4e5104417bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
